{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 21:49:59.965394: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-30 21:49:59.974215: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-30 21:49:59.996855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743367800.033907    4995 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743367800.044849    4995 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743367800.077240    4995 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743367800.077278    4995 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743367800.077285    4995 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743367800.077290    4995 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-30 21:50:00.087680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5051, 2)\n",
      "Columns: Index(['text', 'source'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1743367807.397342    4995 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1743367807.398779    4995 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4040, 600)\n",
      "y_train shape: (4040,)\n",
      "x_test shape: (1011, 600)\n",
      "y_test shape: (1011,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "csv_path = '../../datasets/human_or_ai_dataset_small.csv'  # Change this to your file path\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Parameters\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].values\n",
    "labels = df['source'].values\n",
    "\n",
    "# Convert labels to numeric values\n",
    "label_map = {'human': 0, 'ai': 1}\n",
    "y_data = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Define TextVectorization layer\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "# Adapt to the text dataset\n",
    "text_vectorization.adapt(texts)\n",
    "\n",
    "# Transform text data into tokenized sequences\n",
    "x_data = text_vectorization(texts).numpy()\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Learning\n",
    "Zero-Shot Learning (ZSL) é uma abordagem onde o modelo é capaz de classificar exemplos de classes que nunca viu durante o treino. Para isso, usam-se embeddings pré-treinados e é preciso ajustar o modelo para generalizar para novas classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc BART: 0.88\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# load do modelo e tokenizer do BART\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# fun para obter embeddings BART\n",
    "def get_bart_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bart_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Usa o embedding do primeiro token\n",
    "\n",
    "# descrições das classes\n",
    "human_description = \"Text written by a human is creative and expressive.\"  \n",
    "ai_description = \"Text generated by artificial intelligence is predictable and structured.\"\n",
    "\n",
    "# embeddings das classes\n",
    "human_embedding = get_bart_embedding(human_description)\n",
    "ai_embedding = get_bart_embedding(ai_description)\n",
    "\n",
    "# fun para classificar um texto\n",
    "def classify_text_bart(text):\n",
    "    text_embedding = get_bart_embedding(text)\n",
    "    \n",
    "    # similaridade coseno\n",
    "    human_similarity = np.dot(text_embedding, human_embedding) / (np.linalg.norm(text_embedding) * np.linalg.norm(human_embedding))\n",
    "    ai_similarity = np.dot(text_embedding, ai_embedding) / (np.linalg.norm(text_embedding) * np.linalg.norm(ai_embedding))\n",
    "    \n",
    "    return 0 if human_similarity > ai_similarity else 1  # 0 = humano, 1 = IA\n",
    "\n",
    "# reduzir pras x primeiras amostras\n",
    "df_shorted = df.head(100)\n",
    "\n",
    "y_true = np.array([label_map[label] for label in df_shorted['source'].values])\n",
    "\n",
    "y_pred = [classify_text_bart(text) for text in df_shorted['text'].values]\n",
    "\n",
    "# acc\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"acc BART: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc RoBERTa: 0.44\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# load do modelo e tokenizer do RoBERTa\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# fun para obter embeddings do RoBERTa\n",
    "def get_roberta_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = roberta_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Usa o embedding do primeiro token\n",
    "\n",
    "# obter embeddings das descricoes\n",
    "human_embedding = get_roberta_embedding(human_description)\n",
    "ai_embedding = get_roberta_embedding(ai_description)\n",
    "\n",
    "# fun para classificar um texto\n",
    "def classify_text_roberta(text):\n",
    "    text_embedding = get_roberta_embedding(text)\n",
    "    \n",
    "    # similaridade coseno\n",
    "    human_similarity = np.dot(text_embedding, human_embedding) / (np.linalg.norm(text_embedding) * np.linalg.norm(human_embedding))\n",
    "    ai_similarity = np.dot(text_embedding, ai_embedding) / (np.linalg.norm(text_embedding) * np.linalg.norm(ai_embedding))\n",
    "    \n",
    "    return 0 if human_similarity > ai_similarity else 1  # 0 = humano, 1 = IA\n",
    "\n",
    "\n",
    "y_true = np.array([label_map[label] for label in df_shorted['source'].values])\n",
    "\n",
    "y_pred = [classify_text_roberta(text) for text in df_shorted['text'].values]\n",
    "\n",
    "# acc\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"acc RoBERTa: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc DistilRoBERTa: 0.52\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, DistilBertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# load do modelo e tokenizer do DistilRoBERTa\n",
    "distilroberta_model = DistilBertModel.from_pretrained(\"distilroberta-base\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "\n",
    "# fun para obter embeddings do DistilRoBERTa\n",
    "def get_distilroberta_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = distilroberta_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Usa o embedding do primeiro token\n",
    "\n",
    "# embeddings das descricoes\n",
    "human_embedding = get_distilroberta_embedding(human_description)\n",
    "ai_embedding = get_distilroberta_embedding(ai_description)\n",
    "\n",
    "# fun para classificar um texto\n",
    "def classify_text_distilroberta(text):\n",
    "    text_embedding = get_distilroberta_embedding(text)\n",
    "    \n",
    "    # similaridade coseno\n",
    "    human_similarity = np.dot(text_embedding, human_embedding) / (np.linalg.norm(text_embedding) * np.linalg.norm(human_embedding))\n",
    "    ai_similarity = np.dot(text_embedding, ai_embedding) / (np.linalg.norm(text_embedding) * np.linalg.norm(ai_embedding))\n",
    "    \n",
    "    return 0 if human_similarity > ai_similarity else 1  # 0 = humano, 1 = IA\n",
    "\n",
    "\n",
    "y_true = np.array([label_map[label] for label in df_shorted['source'].values])\n",
    "\n",
    "y_pred = [classify_text_distilroberta(text) for text in df_shorted['text'].values]\n",
    "\n",
    "# acc\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"acc DistilRoBERTa: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot Learning\n",
    "One-Shot Learning (OSL) é uma abordagem onde o modelo é treinado para classificar exemplos de classes com base num único exemplo de cada classe. Isto é útil quando há poucos dados disponíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel, BartTokenizer, BartModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# carregar modelos e tokenizers\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-large\")\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# funcao para obter embeddings\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Embedding do primeiro token\n",
    "\n",
    "\n",
    "# Exemplo de One-Shot de cada classe \n",
    "human_example = df[df['source'] == 'human'].iloc[0]['text']\n",
    "ai_example = df[df['source'] == 'ai'].iloc[0]['text']\n",
    "\n",
    "# Obter embeddings de BERT, RoBERTa e BART\n",
    "human_embedding_bert = get_embedding(human_example, bert_tokenizer, bert_model)\n",
    "ai_embedding_bert = get_embedding(ai_example, bert_tokenizer, bert_model)\n",
    "\n",
    "human_embedding_roberta = get_embedding(human_example, roberta_tokenizer, roberta_model)\n",
    "ai_embedding_roberta = get_embedding(ai_example, roberta_tokenizer, roberta_model)\n",
    "\n",
    "human_embedding_bart = get_embedding(human_example, bart_tokenizer, bart_model)\n",
    "ai_embedding_bart = get_embedding(ai_example, bart_tokenizer, bart_model)\n",
    "\n",
    "# Função para classificar um texto usando BERT\n",
    "def classify_text_bert(text):\n",
    "    text_embedding = get_embedding(text, bert_tokenizer, bert_model)\n",
    "    human_similarity = np.dot(text_embedding, human_embedding_bert) / (np.linalg.norm(text_embedding) * np.linalg.norm(human_embedding_bert))\n",
    "    ai_similarity = np.dot(text_embedding, ai_embedding_bert) / (np.linalg.norm(text_embedding) * np.linalg.norm(ai_embedding_bert))\n",
    "    return 0 if human_similarity > ai_similarity else 1  # 0 = humano, 1 = IA\n",
    "\n",
    "# Função para classificar um texto usando RoBERTa\n",
    "def classify_text_roberta(text):\n",
    "    text_embedding = get_embedding(text, roberta_tokenizer, roberta_model)\n",
    "    human_similarity = np.dot(text_embedding, human_embedding_roberta) / (np.linalg.norm(text_embedding) * np.linalg.norm(human_embedding_roberta))\n",
    "    ai_similarity = np.dot(text_embedding, ai_embedding_roberta) / (np.linalg.norm(text_embedding) * np.linalg.norm(ai_embedding_roberta))\n",
    "    return 0 if human_similarity > ai_similarity else 1  # 0 = humano, 1 = IA\n",
    "\n",
    "# Função para classificar um texto usando BART\n",
    "def classify_text_bart(text):\n",
    "    text_embedding = get_embedding(text, bart_tokenizer, bart_model)\n",
    "    human_similarity = np.dot(text_embedding, human_embedding_bart) / (np.linalg.norm(text_embedding) * np.linalg.norm(human_embedding_bart))\n",
    "    ai_similarity = np.dot(text_embedding, ai_embedding_bart) / (np.linalg.norm(text_embedding) * np.linalg.norm(ai_embedding_bart))\n",
    "    return 0 if human_similarity > ai_similarity else 1  # 0 = humano, 1 = IA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc BERT: 0.95\n",
      "acc RoBERTa: 1.0\n",
      "acc BART: 1.0\n"
     ]
    }
   ],
   "source": [
    "df_shorted = df.head(100)\n",
    "\n",
    "y_true = np.array([label_map[label] for label in df_shorted['source'].values])\n",
    "\n",
    "# 3 models\n",
    "y_pred_bert = [classify_text_bert(text) for text in df_shorted['text'].values]\n",
    "y_pred_roberta = [classify_text_roberta(text) for text in df_shorted['text'].values]\n",
    "y_pred_bart = [classify_text_bart(text) for text in df_shorted['text'].values]\n",
    "\n",
    "# acc\n",
    "accuracy_bert = accuracy_score(y_true, y_pred_bert)\n",
    "accuracy_roberta = accuracy_score(y_true, y_pred_roberta)\n",
    "accuracy_bart = accuracy_score(y_true, y_pred_bart)\n",
    "\n",
    "print(f\"acc BERT: {accuracy_bert}\")\n",
    "print(f\"acc RoBERTa: {accuracy_roberta}\")\n",
    "print(f\"acc BART: {accuracy_bart}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "O Prompt Engineering é a prática de projetar prompts (entradas) de forma que o modelo gere saídas de alta qualidade. A ideia é fornecer ao modelo o contexto e as instruções mais claras e específicas para obter a resposta desejada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc PE: 0.53\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# BERT \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# funcao de classificacao com engenharia de prompt\n",
    "def classify_with_prompt(text):\n",
    "    #prompt = f\"Classifique o seguinte texto como 'humano' ou 'IA': {text}\"\n",
    "    prompt = f\"Classify the following text as 'human' or 'AI': {text}\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    return torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "# 100 primeiras linhas\n",
    "pe_size = 100\n",
    "\n",
    "y_true_pe = np.array([label_map[label] for label in df['source'].head(pe_size)])  # Ajustar conforme a coluna 'source' no seu dataset\n",
    "y_pred_pe = [classify_with_prompt(text) for text in df['text'].head(pe_size)]  # Ajustar conforme a coluna 'text' no seu dataset\n",
    "\n",
    "# acc\n",
    "accuracy = accuracy_score(y_true_pe, y_pred_pe)\n",
    "print(f\"acc PE: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc Few-shot: 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# modelo e tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# engenharia de prompt e few-shot learning\n",
    "def classify_with_few_shot(text, examples):\n",
    "    \n",
    "    prompt = \"Classify the following text as 'human' or 'AI'\\n\" \n",
    "    \n",
    "    for example in examples:\n",
    "        prompt += f\"Text: {example[0]}\\nClass: {example[1]}\\n\"\n",
    "    \n",
    "    prompt += f\"Text: {text}\\nClass:\"\n",
    "\n",
    "    # Tokenizar o prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Fazer a predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    return torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "\n",
    "# Criar exemplos few-shot a partir do dataset\n",
    "# Selecionar 3 exemplos para cada classe (humano e IA)\n",
    "human_examples = df[df['source'] == 'human'].tail(3)[['text', 'source']].values.tolist()\n",
    "ai_examples = df[df['source'] == 'IA'].tail(3)[['text', 'source']].values.tolist()\n",
    "\n",
    "few_shot_examples = []\n",
    "for example in human_examples:\n",
    "    few_shot_examples.append((example[0], \"human\"))\n",
    "for example in ai_examples:\n",
    "    few_shot_examples.append((example[0], \"AI\"))\n",
    "\n",
    "# Testar com x entradas do dataset\n",
    "pe_size = 50  \n",
    "\n",
    "y_true_pe = np.array([1 if label == 'IA' else 0 for label in df['source'].head(pe_size)])  \n",
    "y_pred_pe = [classify_with_few_shot(text, few_shot_examples) for text in df['text'].head(pe_size)]  \n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_true_pe, y_pred_pe)\n",
    "print(f\"acc Few-shot: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc GPT-2: 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2ForSequenceClassification,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPT-2\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "gpt2_model = GPT2ForSequenceClassification.from_pretrained(\"gpt2-large\", num_labels=2).to(device)\n",
    "\n",
    "\n",
    "# Função para classificação few-shot\n",
    "def classify_with_few_shot(text, examples, tokenizer, model):\n",
    "    # Construir o prompt\n",
    "    prompt = \"Classify the following text as 'human' or 'AI'\\n\" \n",
    "    \n",
    "    for example in examples:\n",
    "        prompt += f\"Text: {example[0]}\\nClass: {example[1]}\\n\"\n",
    "    \n",
    "    prompt += f\"Text: {text}\\nClass:\"\n",
    "\n",
    "    # Tokenizar o prompt\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Fazer a predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    return torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "# Criar exemplos few-shot a partir do dataset\n",
    "# Selecionar 3 exemplos para cada classe (humano e IA)\n",
    "human_examples = df[df['source'] == 'human'].tail(3)[['text', 'source']].values.tolist()\n",
    "ai_examples = df[df['source'] == 'ai'].tail(3)[['text', 'source']].values.tolist()\n",
    "\n",
    "few_shot_examples = []\n",
    "for example in human_examples:\n",
    "    few_shot_examples.append((example[0], \"human\"))\n",
    "for example in ai_examples:\n",
    "    few_shot_examples.append((example[0], \"AI\"))\n",
    "\n",
    "# Testar com x entradas do dataset\n",
    "pe_size = 100   \n",
    "\n",
    "y_true_pe = np.array([1 if label == 'IA' else 0 for label in df['source'].head(pe_size)])  \n",
    "\n",
    "y_pred_gpt2 = [classify_with_few_shot(text, few_shot_examples, gpt2_tokenizer, gpt2_model) \n",
    "               for text in df['text'].head(pe_size)]\n",
    "\n",
    "# acc\n",
    "accuracy_gpt2 = accuracy_score(y_true_pe, y_pred_gpt2)\n",
    "print(f\"acc GPT-2: {accuracy_gpt2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (Retrieval-Augmented Generation)\n",
    "O RAG é uma técnica que combina recuperação de informações com geração de texto. Em vez de confiar apenas na capacidade do modelo de gerar respostas com base no seu treino, o modelo também consulta fontes externas de dados (como uma base de dados ou documentos relevantes) para melhorar a qualidade e a precisão das respostas geradas.\n",
    "\n",
    "Não Implementado (justificação no relatório)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
