{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed) # Python\n",
    "    np.random.seed(seed)  # Numpy, Ã© o gerador utilizado pelo sklearn\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # sistema operativo\n",
    "\n",
    "set_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 12:14:39.567988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742213679.912528   16648 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742213680.004996   16648 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742213680.614566   16648 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742213680.614600   16648 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742213680.614603   16648 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742213680.614606   16648 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-17 12:14:40.681015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5051, 2)\n",
      "Columns: Index(['text', 'source'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout, Input, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load dos dados\n",
    "csv_path = '../../datasets/human_or_ai_dataset_small.csv'  # Change this to your file path\n",
    "df = pd.read_csv(csv_path)\n",
    "# Sanity check!\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742213695.457247   16648 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4040, 600)\n",
      "y_train shape: (4040,)\n",
      "x_test shape: (1011, 600)\n",
      "y_test shape: (1011,)\n",
      "Sample sequence: [   15    20     2   680     3  4751   121  6079  3152     6     2   624\n",
      "  1302   524     6     5   216   432     3     1  1533     1  3011   104\n",
      "    15   954     2  2998   505 18556    23  5046  6078     6    38   104\n",
      "     4    56     9   121  6078     8   224    10     2   583   144  8081\n",
      "  3011    44    15   129     9 18556  7232    91     2   627   378     8\n",
      "   150  3530    15    56     9   121  6078     8    31   224     6     5\n",
      "  6757    44    11    24   489   463     7   246    23  1891     6   627\n",
      "    47    67     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
      "Sample label: 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].values\n",
    "labels = df['source'].values\n",
    "\n",
    "# Convert labels to numeric values\n",
    "label_map = {'human': 0, 'ai': 1}\n",
    "y_data = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Define TextVectorization layer\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "# Adapt to the text dataset\n",
    "text_vectorization.adapt(texts)\n",
    "\n",
    "# Transform text data into tokenized sequences\n",
    "x_data = text_vectorization(texts).numpy()  # Convert TensorFlow tensor to NumPy array\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Print a sample\n",
    "print(\"Sample sequence:\", x_train[0])\n",
    "print(\"Sample label:\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 16\n",
    "val_fraction = 0.15\n",
    "seed = 25\n",
    "\n",
    "# Shuffle the entire dataset with a fixed seed\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=len(x_train), seed=seed)\n",
    "\n",
    "# Calculate split sizes\n",
    "val_size = int(len(x_train) * val_fraction)\n",
    "train_size = len(x_train) - val_size\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_ds = dataset.skip(val_size).batch(batch_size)\n",
    "val_ds = dataset.take(val_size).batch(batch_size)\n",
    "\n",
    "# Create test dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "train_ds = dataset.skip(val_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforme com Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "#    def compute_mask(self, inputs, mask=None):\n",
    "#        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³ââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\"> Layer (type)                    </span>â<span style=\"font-weight: bold\"> Output Shape           </span>â<span style=\"font-weight: bold\">       Param # </span>â\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           â             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â positional_embedding_2          â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,273,600</span> â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)           â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â transformer_encoder_2           â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â       <span style=\"color: #00af00; text-decoration-color: #00af00\">543,776</span> â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â global_max_pooling1d_2          â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> â\n",
       "âââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´ââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³ââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           â             \u001b[38;5;34m0\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â positional_embedding_2          â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â     \u001b[38;5;34m5,273,600\u001b[0m â\n",
       "â (\u001b[38;5;33mPositionalEmbedding\u001b[0m)           â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â transformer_encoder_2           â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â       \u001b[38;5;34m543,776\u001b[0m â\n",
       "â (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â global_max_pooling1d_2          â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â             \u001b[38;5;34m0\u001b[0m â\n",
       "â (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â             \u001b[38;5;34m0\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dense_8 (\u001b[38;5;33mDense\u001b[0m)                 â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â           \u001b[38;5;34m257\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´ââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,817,633</span> (22.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,817,633\u001b[0m (22.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,817,633</span> (22.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,817,633\u001b[0m (22.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 881ms/step - accuracy: 0.7038 - loss: 0.7271 - val_accuracy: 0.9307 - val_loss: 0.1811\n",
      "Epoch 2/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 765ms/step - accuracy: 0.9041 - loss: 0.2148 - val_accuracy: 0.9092 - val_loss: 0.2314\n",
      "Epoch 3/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 771ms/step - accuracy: 0.9435 - loss: 0.1393 - val_accuracy: 0.9620 - val_loss: 0.0958\n",
      "Epoch 4/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 784ms/step - accuracy: 0.9532 - loss: 0.1146 - val_accuracy: 0.9719 - val_loss: 0.0753\n",
      "Epoch 5/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 712ms/step - accuracy: 0.9609 - loss: 0.1018 - val_accuracy: 0.9158 - val_loss: 0.1768\n",
      "Epoch 6/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 728ms/step - accuracy: 0.9742 - loss: 0.0687 - val_accuracy: 0.9818 - val_loss: 0.0520\n",
      "Epoch 7/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 783ms/step - accuracy: 0.9777 - loss: 0.0586 - val_accuracy: 0.9868 - val_loss: 0.0288\n",
      "Epoch 8/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 1s/step - accuracy: 0.9865 - loss: 0.0406 - val_accuracy: 0.9901 - val_loss: 0.0249\n",
      "Epoch 9/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 896ms/step - accuracy: 0.9803 - loss: 0.0570 - val_accuracy: 0.9901 - val_loss: 0.0387\n",
      "Epoch 10/10\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 831ms/step - accuracy: 0.9898 - loss: 0.0332 - val_accuracy: 0.9967 - val_loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/daa312/lib/python3.12/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'positional_embedding_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/david/miniconda3/envs/daa312/lib/python3.12/site-packages/keras/src/layers/layer.py:395: UserWarning: `build()` was called on layer 'transformer_encoder_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 324ms/step - accuracy: 0.9486 - loss: 0.1672\n",
      "Test acc: 0.950\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "# Early stopping \n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\", save_best_only=True),\n",
    "    early_stopping\n",
    "]\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 336ms/step - accuracy: 0.9486 - loss: 0.1672\n",
      "Test acc: 0.950\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 707ms/step\n",
      "PrevisÃµes guardadas em output_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = '../../datasets/dataset1_inputs.csv'\n",
    "df_input = pd.read_csv(dataset_path, sep='\\t')\n",
    "\n",
    "texts_input = df_input['Text'].values\n",
    "x_input = text_vectorization(texts_input).numpy()\n",
    "\n",
    "predictions = model.predict(x_input)\n",
    "predicted_labels = np.where(predictions.flatten() >= 0.5, 'AI', 'Human')\n",
    "\n",
    "df_output = pd.DataFrame({\n",
    "    'ID': df_input['ID'],\n",
    "    'Prediction': predicted_labels\n",
    "})\n",
    "\n",
    "output_path = 'output_predictions.csv'\n",
    "df_output.to_csv(output_path, index=False)\n",
    "\n",
    "print(f'PrevisÃµes guardadas em {output_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
