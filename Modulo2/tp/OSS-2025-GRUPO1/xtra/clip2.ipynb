{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20eb3611",
   "metadata": {},
   "source": [
    "# Proposta para OSS2025\n",
    "![Terceira Arquitetura do Grupo](arc2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ba2f9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7079289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import Dropout2d\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Module\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_video\n",
    "import torchvision.io as tvio   \n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Normalize\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_video\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "np.random.seed(0) \n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cde40b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d166bf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "         for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a76a1",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "617df359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStreamVideoDataset(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_dir,\n",
    "        label_path,\n",
    "        clip_preprocess,\n",
    "        num_2d_frames=8,\n",
    "        num_3d_clips=4,\n",
    "        frames_per_clip=16,\n",
    "        transform_3d=None,\n",
    "        label_map=None\n",
    "    ):\n",
    "        self.video_dir = video_dir\n",
    "        self.clip_preprocess = clip_preprocess\n",
    "        self.num_2d_frames = num_2d_frames\n",
    "        self.num_3d_clips = num_3d_clips\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.transform_3d = transform_3d or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ConvertImageDtype(torch.float32)\n",
    "        ])\n",
    "        self.labels_dict = self._load_labels(label_path)\n",
    "\n",
    "        self.videos = [\n",
    "            f for f in os.listdir(video_dir)\n",
    "            if f.endswith(\".mp4\") and os.path.splitext(f)[0] in self.labels_dict\n",
    "        ]\n",
    "        print(f\"Found {len(self.videos)} labeled videos\")\n",
    "\n",
    "        if label_map:\n",
    "            self.label_map = label_map\n",
    "        else:\n",
    "            unique_labels = sorted(set(self.labels_dict.values()))\n",
    "            self.label_map = {name: i for i, name in enumerate(unique_labels)}\n",
    "\n",
    "    def _load_labels(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Label file not found: {path}\")\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            df = pd.read_excel(path, header=0)\n",
    "\n",
    "            if \"VIDEO\" not in df.columns or \"GLOBA_RATING_SCORE\" not in df.columns:\n",
    "                raise ValueError(\"Required columns not found.\")\n",
    "\n",
    "            df = df.drop(columns=[c for c in [\"STUDENT\", \"GROUP\", \"TIME\", \"SUTURES\", \"INVESTIGATOR\"] if c in df.columns])\n",
    "            df = df.groupby(\"VIDEO\").mean()\n",
    "\n",
    "            df[\"TARGET_CLASS\"] = df[\"GLOBA_RATING_SCORE\"].apply(\n",
    "                lambda x: \"novice\" if 8 <= x < 16 else\n",
    "                          \"intermediate\" if 16 <= x < 24 else\n",
    "                          \"proficient\" if 24 <= x < 32 else\n",
    "                          \"expert\" if 32 <= x <= 40 else \"unknown\"\n",
    "            )\n",
    "\n",
    "            df = df[df[\"TARGET_CLASS\"] != \"unknown\"]  # filter unknowns\n",
    "            return dict(zip(df.index, df[\"TARGET_CLASS\"]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading labels: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def _sample_indices(self, total_frames, num_samples):\n",
    "        if total_frames == 0:\n",
    "            raise ValueError(\"Total frames is zero, cannot sample indices.\")\n",
    "        if total_frames < num_samples:\n",
    "            return [i % total_frames for i in range(num_samples)]\n",
    "        return sorted(random.sample(range(total_frames), num_samples))\n",
    "\n",
    "    def _cv2_to_tensor(self, frame):\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        return Image.fromarray(frame_rgb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.videos[idx]\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        label_str = self.labels_dict[video_name]\n",
    "        label = self.label_map[label_str]\n",
    "\n",
    "        video_path = os.path.join(self.video_dir, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        idx_2d = self._sample_indices(total_frames, self.num_2d_frames)\n",
    "\n",
    "        clip_start_indices = []\n",
    "        for _ in range(self.num_3d_clips):\n",
    "            if total_frames < self.frames_per_clip:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                start_idx = random.randint(0, total_frames - self.frames_per_clip)\n",
    "            clip_start_indices.append(start_idx)\n",
    "\n",
    "        clip_indices = []\n",
    "        for start_idx in clip_start_indices:\n",
    "            clip_indices.extend(range(start_idx, start_idx + self.frames_per_clip))\n",
    "        all_needed_indices = sorted(set(idx_2d + clip_indices))\n",
    "\n",
    "        frame_dict = {}\n",
    "        current_frame = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if current_frame in all_needed_indices:\n",
    "                frame_dict[current_frame] = self._cv2_to_tensor(frame)\n",
    "            current_frame += 1\n",
    "            if current_frame > max(all_needed_indices) and len(frame_dict) == len(all_needed_indices):\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        frames_2d = [self.clip_preprocess(frame_dict.get(i, frame_dict[max(frame_dict.keys())])) for i in idx_2d]\n",
    "        frames_2d = torch.stack(frames_2d)\n",
    "\n",
    "        clips_3d = []\n",
    "        for start_idx in clip_start_indices:\n",
    "            clip_frames = [frame_dict.get(i, frame_dict[max(frame_dict.keys())]) for i in range(start_idx, start_idx + self.frames_per_clip)]\n",
    "            processed_frames = [self.transform_3d(transforms.ToTensor()(f)) for f in clip_frames]\n",
    "            clip = torch.stack(processed_frames).permute(1, 0, 2, 3)  # (C, T, H, W)\n",
    "            clips_3d.append(clip)\n",
    "\n",
    "        clips_3d = torch.stack(clips_3d)  # (S, C, T, H, W)\n",
    "\n",
    "        return frames_2d, clips_3d, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73fa754",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdd22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 labeled videos\n",
      "Train: 9 samples, Validation: 3 samples, Test: 2 samples\n"
     ]
    }
   ],
   "source": [
    "def create_data_loaders(dataset, train_idx, val_idx, test_idx, batch_size=4):\n",
    "    \"\"\"Create data loaders for train, validation and test sets.\"\"\"\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        sampler=val_sampler,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        sampler=test_sampler,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Optional: Set smaller batch sizes and frame counts to reduce memory usage\n",
    "BATCH_SIZE = 2  # Try smaller batch size first\n",
    "NUM_2D_FRAMES = 8  # Reduce from 8\n",
    "NUM_3D_CLIPS = 4   # Reduce from 4\n",
    "FRAMES_PER_CLIP = 16  # Reduce from 16\n",
    "\n",
    "_, _, clip_preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained='laion2b_s34b_b79k')\n",
    "\n",
    "dataset = MultiStreamVideoDataset(\n",
    "    video_dir=\"Package_train\",\n",
    "    label_path=\"OSATS.xlsx\",\n",
    "    clip_preprocess=clip_preprocess,\n",
    "    num_2d_frames=NUM_2D_FRAMES,  # Adjust based on memory constraints\n",
    "    num_3d_clips=NUM_3D_CLIPS,   # Adjust based on memory constraints\n",
    "    frames_per_clip=FRAMES_PER_CLIP, # Optimal for R3D models\n",
    "    label_map={\n",
    "        \"novice\": 0,\n",
    "        \"intermediate\": 1,\n",
    "        \"proficient\": 2,\n",
    "        \"expert\": 3\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create train/val/test splits - 70%/20%/10%\n",
    "indices = list(range(len(dataset)))\n",
    "# Ideally, you would want to stratify based on the labels\n",
    "# train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=0, stratify=[dataset.labels_dict[os.path.splitext(dataset.videos[i])[0]] for i in indices])\n",
    "# val_idx, test_idx = train_test_split(temp_idx, test_size=0.33, random_state=0, stratify=[dataset.labels_dict[os.path.splitext(dataset.videos[i])[0]] for i in temp_idx])\n",
    "\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=0)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.33, random_state=0)\n",
    "\n",
    "print(f\"Train: {len(train_idx)} samples, Validation: {len(val_idx)} samples, Test: {len(test_idx)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    dataset, train_idx, val_idx, test_idx, BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4ade0",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858cd5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics from the MICCAI24 package\n",
    "from metrics4MICCAI24.expected_cost import get_expected_cost\n",
    "from metrics4MICCAI24.utils import get_multi_class_confusion_matrix\n",
    "from metrics4MICCAI24.utils import load_csv, get_multiclass_tp_tn_fp_fn\n",
    "\n",
    "\n",
    "def get_f1(data_df, num_classes=4):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score for the given data.\n",
    "    Args:\n",
    "        data_df: dataframe with columns 'item_id', 'ground_truth', 'prediction'.\n",
    "        num_classes: number of classes.\n",
    "\n",
    "    Returns:\n",
    "        float: The F1 score.\n",
    "    \"\"\"\n",
    "    tp, tn, fp, fn, _ = get_multiclass_tp_tn_fp_fn(data_df, num_classes)\n",
    "    f1_scores = []\n",
    "    for i in range(num_classes):\n",
    "        if tp[i] + fp[i] > 0 and tp[i] + fn[i] > 0 and tp[i] > 0:\n",
    "            precision = tp[i] / (tp[i] + fp[i])\n",
    "            recall = tp[i] / (tp[i] + fn[i])\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            f1_scores.append(f1)\n",
    "        else:\n",
    "            f1_scores.append(0)\n",
    "    print(f1_scores)\n",
    "    return sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Adapter functions to work with MICCAI24 metrics package\n",
    "def _create_metrics_dataframe(labels, predictions):\n",
    "    # Create dummy item_ids\n",
    "    item_ids = list(range(len(labels)))\n",
    "    \n",
    "    data = {\n",
    "        'item_id': item_ids,\n",
    "        'ground_truth': labels,\n",
    "        'prediction': predictions\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, num_classes=4):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for frames_2d, clips_3d, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        frames_2d = frames_2d.to(device)\n",
    "        clips_3d = clips_3d.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(frames_2d, clips_3d)  # <-- no prompts\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * frames_2d.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    data_df = _create_metrics_dataframe(all_labels, all_predictions)\n",
    "    f1_score = get_f1(data_df, num_classes)\n",
    "    expected_cost = get_expected_cost(data_df, num_classes)\n",
    "\n",
    "    return epoch_loss, f1_score, expected_cost\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, num_classes=4):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames_2d, clips_3d, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            frames_2d = frames_2d.to(device)\n",
    "            clips_3d = clips_3d.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(frames_2d, clips_3d)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * frames_2d.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    data_df = _create_metrics_dataframe(all_labels, all_predictions)\n",
    "    f1_score = get_f1(data_df, num_classes)\n",
    "    expected_cost = get_expected_cost(data_df, num_classes)\n",
    "\n",
    "    return epoch_loss, f1_score, expected_cost\n",
    "\n",
    "\n",
    "def test(model, dataloader, device, num_classes=4):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    item_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_idx = 0\n",
    "        for frames_2d, clips_3d, labels in tqdm(dataloader, desc=\"Testing\"):\n",
    "            frames_2d = frames_2d.to(device)\n",
    "            clips_3d = clips_3d.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(frames_2d, clips_3d)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            batch_item_ids = list(range(batch_idx * frames_2d.size(0), (batch_idx + 1) * frames_2d.size(0)))\n",
    "            item_ids.extend(batch_item_ids)\n",
    "            batch_idx += 1\n",
    "\n",
    "    data_df = _create_metrics_dataframe(all_labels, all_predictions)\n",
    "    f1_score = get_f1(data_df, num_classes)\n",
    "    expected_cost = get_expected_cost(data_df, num_classes)\n",
    "\n",
    "    return f1_score, expected_cost, all_predictions, all_labels, item_ids\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    test_loader,\n",
    "    num_epochs=5, \n",
    "    lr=0.001, \n",
    "    weight_decay=1e-4,  # l2\n",
    "    save_dir='checkpoints',\n",
    "    num_classes=4 \n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    # Set up\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    #scheduler = scheduler.ReduceLROnPlateau(\n",
    "    #   optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    #)\n",
    "    \n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize LiveLossPlot\n",
    "    liveloss = PlotLosses()\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_f1, train_cost = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, num_classes\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_f1, val_cost = validate(\n",
    "            model, val_loader, criterion, device, num_classes\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        # scheduler.step(val_loss)\n",
    "        \n",
    "        # Log metrics for LiveLossPlot\n",
    "        logs = {}\n",
    "        logs['loss'] = train_loss\n",
    "        logs['val_loss'] = val_loss\n",
    "        logs['f1'] = train_f1\n",
    "        logs['val_f1'] = val_f1\n",
    "        logs['cost'] = train_cost\n",
    "        logs['val_cost'] = val_cost\n",
    "        \n",
    "        # Display logs\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, F1: {train_f1:.4f}, Cost: {train_cost:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Cost: {val_cost:.4f}\")\n",
    "        \n",
    "        # Save model after each epoch\n",
    "        \"\"\"\n",
    "        checkpoint_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_f1': train_f1,\n",
    "            'val_f1': val_f1,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Model saved to {checkpoint_path}\")\n",
    "        \"\"\"\n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_f1': train_f1,\n",
    "                'val_f1': val_f1,\n",
    "            }, best_model_path)\n",
    "            print(f\"Best model saved to {best_model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def save_predictions_to_csv(item_ids, predictions, filename):\n",
    "    \"\"\"\n",
    "    Save predictions to a CSV file in the format expected by MICCAI24.\n",
    "    \n",
    "    Args:\n",
    "        item_ids: List of item IDs.\n",
    "        predictions: List of predicted labels.\n",
    "        filename: Name of the output file.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'item_id': item_ids,\n",
    "        'prediction': predictions\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")\n",
    "\n",
    "# Function to visualize confusion matrix\n",
    "def plot_confusion_matrix(labels, predictions, num_classes=5, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for the predictions.\n",
    "    \n",
    "    Args:\n",
    "        labels: Ground truth labels.\n",
    "        predictions: Predicted labels.\n",
    "        num_classes: Number of classes.\n",
    "        figsize: Figure size (width, height).\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Create DataFrame for MICCAI24 metrics\n",
    "    data_df = _create_metrics_dataframe(labels, predictions)\n",
    "    \n",
    "    # Get confusion matrix\n",
    "    cm = get_multi_class_confusion_matrix(data_df, num_classes)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdcef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ClipStyleMultiStreamClassifier\n",
    "\n",
    "model = ClipStyleMultiStreamClassifier(class_prompts=[\n",
    "  \"a video of a novice surgeon performing suturing\",\n",
    "  \"a video of an intermediate surgeon suturing\",\n",
    "  \"a video of a proficient surgeon suturing\",\n",
    "  \"a video of an expert surgeon suturing\"\n",
    "])\n",
    "# train the model\n",
    "# make a prediction on a random video\n",
    "# BATCH_SIZE = 2  # Try smaller batch size first\n",
    "# NUM_2D_FRAMES = 8  # Reduce from 8\n",
    "# NUM_3D_CLIPS = 4   # Reduce from 4\n",
    "# FRAMES_PER_CLIP = 16  # Reduce from 16\n",
    "save_dir = 'checkpoints2'\n",
    "frames_2d = torch.randn(BATCH_SIZE, NUM_2D_FRAMES, 3, 224, 224)       # (B, T, C, H, W)\n",
    "clips_3d  = torch.randn(BATCH_SIZE, NUM_3D_CLIPS, 3, FRAMES_PER_CLIP, 224, 224)   # (B, S, C, T, H, W)\n",
    "print(summary(model, input_data=[frames_2d, clips_3d], depth=3, col_names=[\"input_size\", \"output_size\", \"num_params\"]))\n",
    "model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=3,\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    save_dir=save_dir,\n",
    "    num_classes=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test\n",
    "print(\"\\nTesting best model...\")\n",
    "# Load the best model\n",
    "# checkpoint = torch.load(save_dir + \"/model_epoch_1.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_f1, test_cost, test_preds, test_labels, item_ids = test(model, test_loader, device, 4)\n",
    "print(f\"Test F1: {test_f1:.4f}, Test Cost: {test_cost:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
